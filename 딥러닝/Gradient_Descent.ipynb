{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsuCJBDH+FdSBoyGL6naPb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Georgia-MAX-holic/theory/blob/main/%EB%94%A5%EB%9F%AC%EB%8B%9D/Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SGD (Stochastic Gradient Descent) 와 Mini-Batch GD **\n",
        "\n",
        "- Gradient Descent 방식은 전체 학습 데이터를 기반으로 GD를 계산. 하지만 입력 데이터가 크고 레이어가 많을 수록 GD를 계산하는데 많은 Computing 자원이 소모( 일반적으로 메모리 부족으로 연산 불가능할 수 있음)\n",
        "\n",
        "- 이를 극복하기 위해 Stochastic Gradient Descent와 Mini-Batch Gradient Descent 방식이 도입 \n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "wIo83-SSLleV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRdLYyeeJ-kB"
      },
      "outputs": [],
      "source": [
        "GD(Gradient Descent)        | SGD                               | Mini -Batch GD \n",
        "-----------------------------------------------------------------------\n",
        "전체 학습 데이터를 기반으로 |전체 학습 데이터 중                |전체 학습 데이터 중 특정 크기 만큼(Batch 크기)\n",
        "GD를 계산                   | 한 건만 임의로 선택하여 GD를 계산 |임의로 선택해서 GD 계산\n",
        "\n",
        "\n",
        "[일반적으로 Mini-Batch GD가 대부분의 딥러닝 Framework에서 채택됨 ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Batch Gradient Descent \n",
        " - 전체 학습 데이터의 Gradient를 평균내어 GD의 매번 step마다 적용 \n",
        "\n",
        "2. Stochastic Gradient Descent \n",
        " - 데이터 셋이 매우 큰경우, 매번 전체 학습 데이터를 연산하는 것은 메모리 사용량이 늘어나는 일 \n",
        "\n",
        " - 하나의 데이터를 골라 인공신경망에 학습시킴 \n",
        "\n",
        " - Full batch와는 다르게 확률론적, 불확실성의 개념을 도입 \n",
        "\n",
        "\n",
        "optimiztion.의 loop \n",
        "\n",
        " - Take an example\n",
        " - Feed it to Neural Netwrok\n",
        " - Calculate it's gradient\n",
        " - Use the gradient we calculated in step 3 to update the weights\n",
        " - Repeat steps 1-4 for all the examples in training dataset \n",
        "\n",
        "\n",
        "\n",
        "3. SGD 장점 \n",
        "\n",
        "  - SGD는 빠른 속도로 수렴 \n",
        "  - SGD는 local optima에 빠질 위험이 적음 \n",
        "\n",
        "\n",
        "4. mini-Batch Gradient Descent \n",
        "\n",
        " - 위 두 방법의 절충안 \n",
        " - 전체 학습 데이터를 minibatch로 나누어, GD 진행 \n",
        "\n",
        "optimization의 loop 는 이와 같다\n",
        "\n",
        " - Pick a mini-batch (하나의 데이터가 아닌)\n",
        " - Feed it to Neural Network\n",
        " - Calculate the mean gradient of the mini-batch (batch GD의 특성 적용)\n",
        "\n",
        " - Use the mean gradient we caculated in step 3 to update the weights \n",
        "\n",
        " -Repeat steps 1-4 for the mini-batches we created \n",
        "\n",
        "\n",
        " minibatch GD 장점\n",
        "\n",
        " - SGD 에 비해 local optima에 빠질 위험이 줄어듬 \n",
        " - 병렬처리에 유리(Batch를 나누니까)\n",
        " - Batch GD보다 메모리 사용을 절약 가능 "
      ],
      "metadata": {
        "id": "sHB12ce-Sviq"
      }
    }
  ]
}