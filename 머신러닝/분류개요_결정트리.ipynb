{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyDmG+lHCGCp+cD+WmQrG/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Georgia-MAX-holic/theory/blob/main/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/%EB%B6%84%EB%A5%98%EA%B0%9C%EC%9A%94_%EA%B2%B0%EC%A0%95%ED%8A%B8%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 분류 알고리즘 \n",
        "\n",
        " 분류(Classification)은 학습 데이터로 주어진 데이터의 피처와 레이블값(결정 값, 클래스 값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 이렇게 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블값을 예측하는 것 \n",
        "\n",
        "\n",
        " - 베이즈(Bayes)통계와 생성 모델에 기반한 나이브 베이즈(Naive Bayes)\n",
        "\n",
        " - 독립변수와 종속변수의 선형 관계성에 기반한 로지스틱 회귀\n",
        "\n",
        " - 데이터 균일도에 따른 규칙 기반의 결정 트리(Decision Tree)\n",
        "\n",
        " - 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신 (Support Vector Machine)\n",
        "\n",
        " - 근접 거리를 기준으로 하는 최소 근접 (Nearest Neighbor) 알고리즘\n",
        "\n",
        " - 심층 연결 기반의 신경망(Neural Network)\n",
        "\n",
        " - 서로 다른(또는 같은) 머신러닝 알고리즘을 결합한 앙상블(Ensemble)"
      ],
      "metadata": {
        "id": "9wGjX9nFX7Dt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# 결정트리 \n",
        "- **결정 트리**: 매우 쉽고 유연하게 적용될 수 있는 알고리즘. 또한 데이터의 스케일링이나 정규화 등의 사전 가공의 영향이 매우 적음. 하지만 예측 성능을 향상시키기 위해 복잡한 규칙 구조를 가져야 하며, 이로 인한 과적합(Overfitting)이 발생해 예측 성능이 저하될 수 도 있음 \n",
        "\n",
        "\n",
        "\n",
        "- 하지만 앙상블 기법에서는 장점으로 작용.\n",
        "앙상블은 매우 많은 여러개의 약한 학습기( 예측 서응이 상대적으로 떨어지는 학습 알고리즘)을 결합해 확률적 보완과 오류가 발생한 부분에 대한 가중치를 계속 업데이트 하면서 예측 성능을 향상시킴. 이는 결정 트리가 좋은 약한 학습기가 되기 때문\n",
        "\n",
        "- 결정 트리 알고리즘은 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만듬 (If-Else 기반 규칙) \n",
        "\n",
        "- 따라서 데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우함 "
      ],
      "metadata": {
        "id": "9LHZnPghjtI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# 정보 균일도 측정 방법 \n",
        "\n",
        "#### 정보 이득(Information Gain) \n",
        "\n",
        "- 정보이득 : 엔트로피라는 개념을 기반으로 함 . 엔트로피는 주어진 데이터 집합의 혼잡도를 의미,  서로 다른 값이 섞여있으면 엔트로피가 높고, 같은 값이 섞여있으면 엔트로피가 낮음. 정보 이득 지수는 1에서 엔트로피 지수를 뺀 값, 즉 1-엔트로피 지수. 결정 트리는 이 정보 이득 지수로 분할 기준을 정함. 즉 정보 이득이 높은 속성을 기준으로 분할 \n",
        "\n",
        "#### 지니 계수 \n",
        "\n",
        "- 지니계수 : 원래 경제학에서 불평등 지수를 나타낼 때 사용하던 계수 .\n",
        "0이 가장 평등하고 1로 갈수록 불평등함. 머신러닝에 적용될 때는 지니 계수가 낮을 수록 데이터 균일도가 높은 것으로 해석되어 계수가 낮은 속성을 기준으로 분할 \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U7zvVyigpS2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 결정 트리 주요 하이퍼 파라미터\n",
        "\n",
        "#### **max_depth**  \n",
        "- 트리의 최대 깊이를 규정 \n",
        "- 디폴트는 None, None 으로 설정하면 완벽하게 클래스 결정 값이 될 때까지 깊이를 계속 키우며 분할하거나 노드가 가지는 데이터 개수가 min_sample_split보다 작아질 때까지 계속 깊이를 증가시킴 \n",
        "- 깊이가 깊어지면 min_samples_split 설정대로 최대 분할하여 과적합할 수 있으므로 적절한 값으로 제어 필요 \n",
        "\n",
        "#### **max_features**\n",
        "- 최적의 분할을 위해 고려할 최대 피처 개수 , 디폴트는 None 데이터 세트의 모든 피처를 사용해 분할 수행 \n",
        "\n",
        "- int 형으로 지정하면 대상 피처의 개수 , float 형으로 지정하면 전체 피처 중 대상 피처의 퍼센트 \n",
        "\n",
        "- sqrt는 전체 피처 중 sqrt(전체 피처 개수), 즉 루트[전체 피처] 개수 만큼 선정 \n",
        "\n",
        "- auto로 지정하면 sqrt 와 동일 \n",
        "\n",
        "- log 는 전체 피처 중 log2(전체 피처 개수) 선정 \n",
        "\n",
        "- None 은 전체 피처 선정 \n",
        "\n",
        "#### **min_samples_split** \n",
        "\n",
        "- 노드를 분할하기 위한 최소한의 샘플 데이터 수, 과적합을 제어하는 데 사용됨 \n",
        "\n",
        "- 디폴트는 2이고 작게 설정할수록 분할되는 노드가 많아져서 과적합 가능성 증가 \n",
        "\n",
        "- 과적합을 제어, 1로 설정할 경우 분할되는 노드가 많아져서 과적합 가능성 증가 \n",
        "\n",
        "#### **min_samples_leaf** \n",
        "- 말단 노드(Leaf)가 되기 위한 최소한의 샘플 데이터 수 \n",
        "- Min_Samples_split와 유사하게 과적합 제어 용도, 그러나 비대칭적(imbalanced) 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 이쓰므로 이 경우 작게 설정 필요 \n",
        "\n",
        "#### **max_loof_nodes **\n",
        "\n",
        "- 말단 노드(Leaf)의 개수 "
      ],
      "metadata": {
        "id": "r1C148CUxg8p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQHeL8u4Xl49"
      },
      "outputs": [],
      "source": []
    }
  ]
}