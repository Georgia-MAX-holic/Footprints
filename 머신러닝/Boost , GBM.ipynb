{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnn28aqZuK8C9Ge+yQi106",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Georgia-MAX-holic/theory/blob/main/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/Boost%20%2C%20GBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 부스팅 (Boosting)\n",
        "\n",
        "- 부스팅 알고리즘은 여러 개의 약한 학습기(Weak learner)를 순차적으로 학습-예측하면서 잘못 예측한 데이터나 학습 트리에 가중치를 부여하여 오류를 개선해 나가는 학습 방식 \n",
        "\n",
        "- 부스팅의 대표적 구현은 AdaBoost(Adaptive boosting)과 그래디언트 부스트가 있음 "
      ],
      "metadata": {
        "id": "_ujujZnFtLWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### GBM(Gradient Boost Machine) 개요\n",
        "\n",
        "- 가중치 업데이트를 경사(Gradient Descent)를 이용하는것이 특징 \n",
        "\n",
        "- 오류 값 : 실제값 - 예측값 , 오류식 h(x) = y-F(x) 이를 최소화 하는 방향으로 \n",
        "\n",
        "- 경사 하강법 : 반복 수행을 통해 오류를 최소화 할 수 있도록 가중치의 업데이트 값을 도출하는 기법 "
      ],
      "metadata": {
        "id": "qpRvJLlSttDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "### GBM 주요 하이퍼 파라미터 및 튜닝 \n",
        "\n",
        "- loss : 경사 하강법에서 사용할 비용 함수를 지정함. 기본값은 deviance\n",
        "\n",
        "- learning_rate : GBM이 학습을 진행할 때마다 적용하는 학습률. Weak learner가 순차적으로 오류값을 보정해나가는데 적용하는 계수 . 0~1 사이의 값을 지정할수 있으며 기본값은 0.1 \n",
        "\n",
        "   1. 너무 작은 값을 적용하면 업데이트 되는 값이 작아져 최소 오류 값을 찾아 예측 성능이 높아질 가능성이 높음. 하지만 많은 weak learner는 순차적인 반복이 필요하여 수행시칸이 오래걸림 \n",
        "\n",
        "   2. 너무 작게 설정하면 모든 weak learner의 반복이 완료되어도 최소 오류 값을 찾이 못할 수 있음 .\n",
        "\n",
        "   3.  반대로 큰 값을 적용하면 최소 오류값을 찾지 못하고 그냥 지나쳐 버려 예측 성능이 떨어질 가능성이 높이지지만 빠른 수행이 가능\n",
        "\n",
        "- n_estimators : weak learner의 개수 . weak learner가 순차적으로 오류를 보정하므로 개수가 많을수록 예측 성능이 일정 수준까지는 좋아질 수 있음. 하지만 개수가 많을수록 수행 시간이 오래 걸림. 기본값은 100\n",
        "\n",
        "- subsample : weak learner가 학습에 사용하는 데이터의 샘플링 비율. 기본값은 1이며, 이는 전체 학습 ㄷ이터를 기반으로 학습한다는 것 . 과적합이 염려되는 경우 subsample을 1보다 작은 값으로 설정 "
      ],
      "metadata": {
        "id": "2YaxGgBxIpPT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi57MK-lszN-"
      },
      "outputs": [],
      "source": [
        "### 부스팅 (Boosting)"
      ]
    }
  ]
}